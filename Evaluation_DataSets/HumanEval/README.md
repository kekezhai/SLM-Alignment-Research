
---
displayName: HumanEval
license:
  - MIT
taskTypes:
  - Image Classification
  - Code Generation
  - Program Synthesis
mediaTypes:
  - Text
labelTypes: []
tags: []
publisher:
  - OpenAI
  - Anthropic AI
  - Zipline
publishDate: '2021-07-01'
publishUrl: https://github.com/openai/human-eval
paperUrl: ''

---
  ## 简介
  这是论文“Evaluating Large Language Models Trained on Code”中描述的 HumanEval 问题解决数据集的评估工具。它用于测量从文档字符串合成程序的功能正确性。它由 164 个原始编程问题组成，评估语言理解、算法和简单的数学，还有一些
类似于简单的软件面试问题。
  ## 引文
  ```
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
```
  
## Download dataset
:modelscope-code[]{type="git"}